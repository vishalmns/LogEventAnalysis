{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a90324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these libraries separately as they are not part of container\n",
    "#!pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all relevant libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbce27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\"\n",
    "\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    TimestampType,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "import math\n",
    "import numpy\n",
    "import random\n",
    "import pyspark.sql.functions as func\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructField,\n",
    "    StructType,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    TimestampType,\n",
    "    BooleanType,\n",
    ")\n",
    "from pyspark.sql.functions import date_trunc, col\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf0b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our apache spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = \"test_topic\"\n",
    "app_name = \"anomaly_app\"\n",
    "kafka_servers = \"broker:9092\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local\")\n",
    "    .appName(app_name)\n",
    "    # Add kafka package\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12-3.0.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant variables which will be used in the code later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_schema = StructType(\n",
    "    [\n",
    "        StructField(\"timestamp\", LongType(), False),\n",
    "        StructField(\"serviceName\", StringType(), False),\n",
    "        StructField(\"logMessage\", StringType(), False),\n",
    "        StructField(\"statusCode\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "        StructField(\"service_id\", StringType(), False),\n",
    "        StructField(\"log_message\", StringType(), True),\n",
    "        StructField(\"status_code\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Size of the sliding window\n",
    "window_size_mins = 3\n",
    "window_size = window_size_mins * 60\n",
    "window_size_millisec = window_size * 1000\n",
    "\n",
    "# Period of the sliding window\n",
    "# In this case wnindow will slide by 1 min and has a size of 3 min\n",
    "window_period = 60\n",
    "window_period_millisec = window_period * 1000\n",
    "\n",
    "\n",
    "total_mins_to_look_back = window_size_mins*3\n",
    "rolling_time_in_seconds = lambda minute: minute * 60\n",
    "offset_window_size = 200\n",
    "\n",
    "# List of service_ids for which we have to detect anomalies\n",
    "service_ids = [\"service_three\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35055cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_kafka_to_service_df(df):\n",
    "    \"\"\"\n",
    "    Function convert kafka output in usable format\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"key\", df[\"key\"].cast(StringType())).withColumn(\n",
    "        \"value\", df[\"value\"].cast(StringType())\n",
    "    )\n",
    "    dfJSON = df.withColumn(\"jsonData\", from_json(col(\"value\"), stream_schema)).select(\n",
    "        \"jsonData.*\"\n",
    "    )\n",
    "    return dfJSON\n",
    "\n",
    "\n",
    "def read_from_kafka(start_offset, end_offset):\n",
    "    \"\"\"\n",
    "    Helper function to read data from kafka, it reads data\n",
    "    between the start_offset and end_offset\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        spark\n",
    "        # .readStream\n",
    "        .read.format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_servers)  # kafka server\n",
    "        .option(\"subscribe\", topic_name)  # topic\n",
    "        .option(\n",
    "            \"startingOffsets\", \"\"\"{\"test_topic\":{\"0\":\"\"\" + str(start_offset) + \"\"\"}}\"\"\"\n",
    "        )  # start from beginning\n",
    "        .option(\n",
    "            \"endingOffsets\", \"\"\"{\"test_topic\":{\"0\":\"\"\" + str(end_offset) + \"\"\"}}\"\"\"\n",
    "        )  # read only 20 records\n",
    "        .load()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    \"\"\"\n",
    "    Function to do basic preprocessing of the dataframe\n",
    "    It standardizes schema and converts epoch time to\n",
    "    timestamp which is used in anomaly detector model\n",
    "    \"\"\"\n",
    "    df = df.withColumnRenamed(\"serviceName\", \"service_id\")\n",
    "    df = df.withColumnRenamed(\"statusCode\", \"status_code\")\n",
    "    df = df.withColumnRenamed(\"logMessage\", \"log_message\")\n",
    "    df = df.withColumn(\n",
    "        \"timestamp\",\n",
    "        func.to_utc_timestamp(\n",
    "            func.from_unixtime(func.col(\"timestamp\") / 1000, \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            \"UTC\",\n",
    "        ),\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def remove_timestamp_window(timestamp_min_window):\n",
    "    return timestamp_min_window[0]\n",
    "\n",
    "\n",
    "timestamp_min_udf = func.udf(\n",
    "    lambda timestamp_min_window: remove_timestamp_window(timestamp_min_window),\n",
    "    TimestampType(),\n",
    ")\n",
    "\n",
    "\n",
    "def check_none(x, y, z):\n",
    "    if x is None or y is None or z is None:\n",
    "        return True\n",
    "    if math.isnan(x) or math.isnan(y) or math.isnan(z):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_anomaly_std(traffic_count, rolling_mean, rolling_std):\n",
    "    \"\"\"\n",
    "    Function to check if the current window is an anomaly, it is based\n",
    "    on the principle that if the currrent traffic count is 2 standard deviation\n",
    "    away from the rolling mean, that means it is an anomaly\n",
    "    \"\"\"\n",
    "    if check_none(traffic_count, rolling_mean, rolling_std):\n",
    "        return False\n",
    "    if rolling_std > 0.2 * rolling_mean and traffic_count < rolling_mean:\n",
    "        return True\n",
    "    if (\n",
    "        (rolling_mean - 2 * rolling_std)\n",
    "        <= traffic_count\n",
    "        <= (rolling_mean + 2 * rolling_std)\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "is_anomaly_udf = func.udf(is_anomaly_std, BooleanType())\n",
    "\n",
    "\n",
    "def get_empty_df(start_time, end_time):\n",
    "    \"\"\"\n",
    "    Used to get empty dataframe between two time stamps, used to\n",
    "    raise anomaly in case of no data from the kakfa\n",
    "    \"\"\"\n",
    "    start_time = datetime.utcfromtimestamp(start_time / 1000)\n",
    "    end_time = datetime.utcfromtimestamp(end_time / 1000)\n",
    "    \n",
    "    df_service_ids = pd.DataFrame(service_ids, columns=[\"service_id\"])\n",
    "    df_service_ids = spark.createDataFrame(df_service_ids)\n",
    "    df_service_ids.registerTempTable(\"temp_window_service\")\n",
    "        \n",
    "    times = pd.date_range(start_time, end_time,freq='Min')\n",
    "    df_times = pd.DataFrame()\n",
    "    df_times['timestamp_min'] = times\n",
    "    df_times.timestamp_min = df_times.timestamp_min.dt.round('min')\n",
    "    df_time_empty = spark.createDataFrame(df_times)\n",
    "    df_time_empty.registerTempTable(\"temp_window_temp_time\")\n",
    "        \n",
    "    df_time_all = spark.sql(\"select service_id, timestamp_min, 0 as traffic_count from temp_window_service, temp_window_temp_time\")\n",
    "    return df_time_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8208410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Skeleton class of the anomaly detector\n",
    "    Every anomaly detector needs to have these\n",
    "    3 functions\n",
    "    \"\"\"\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def fit():\n",
    "        pass\n",
    "\n",
    "    def detect_anomaly():\n",
    "        pass\n",
    "\n",
    "\n",
    "class VanillaAnomalyDetector(BaseAnomalyDetector):\n",
    "    \"\"\"\n",
    "    This anomaly detector works on the principle of threshold.\n",
    "    If the traffic count of any service goes below this threshold,\n",
    "    it will raise an alert\n",
    "    \"\"\"\n",
    "    def __init__(self, traffic_threshold=100):\n",
    "        self.traffic_threshold = traffic_threshold\n",
    "        emptyRDD = spark.sparkContext.emptyRDD()\n",
    "        agg_schema = StructType(\n",
    "            [\n",
    "                StructField(\"service_id\", StringType(), False),\n",
    "                StructField(\"timestamp_min\", TimestampType(), False),\n",
    "                StructField(\"traffic_count\", IntegerType(), False),\n",
    "            ]\n",
    "        )\n",
    "        self.df_cached = spark.createDataFrame(emptyRDD, agg_schema)\n",
    "\n",
    "    def preprocess_df(self, X, start_time, end_time):\n",
    "        \"\"\"\n",
    "        X = (time, status_code, service_id)\n",
    "        Group by at every minute and count traffic for every service\n",
    "        \"\"\"\n",
    "        df_recent = X.withColumn(\n",
    "            \"timestamp_min\", date_trunc(\"minute\", col(\"timestamp\"))\n",
    "        )\n",
    "        df_recent = df_recent.groupBy([\"service_id\", \"timestamp_min\"]).agg(\n",
    "            func.count(\"status_code\").alias(\"traffic_count\")\n",
    "        )\n",
    "        df_recent = df_recent.union(self.df_cached)\n",
    "\n",
    "        df_recent = df_recent.groupBy([\"service_id\", \"timestamp_min\"]).agg(\n",
    "            func.max(\"traffic_count\").alias(\"traffic_count\")\n",
    "        )\n",
    "        self.df_cached = df_recent\n",
    "        self.df_cached = self.df_cached.cache()\n",
    "\n",
    "        return df_recent\n",
    "\n",
    "    def detect_anomaly(self, X, start_time, end_time):\n",
    "        df = self.preprocess_df(X, start_time, end_time)\n",
    "\n",
    "        df = df.groupBy([\"service_id\"]).agg(\n",
    "            func.sum(\"traffic_count\").alias(\"traffic_count\")\n",
    "        )\n",
    "        dict_summ = list(map(lambda row: row.asDict(), df.collect()))\n",
    "        service_errors = []\n",
    "        for service in dict_summ:\n",
    "            if service[\"traffic_count\"] < self.traffic_threshold:\n",
    "                service[\"traffic_threshold\"] = self.traffic_threshold\n",
    "                service_errors.append(service)\n",
    "        return service_errors\n",
    "\n",
    "\n",
    "class RollingAnomalyDetector(BaseAnomalyDetector):\n",
    "    \"\"\"\n",
    "    This anomaly detector is an improvement over vanilla anomaly detector,\n",
    "    here we do not define threshold manually, rather it is calcualted as the\n",
    "    mean of last 'n' windows\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        emptyRDD = spark.sparkContext.emptyRDD()\n",
    "        agg_schema = StructType(\n",
    "            [\n",
    "                StructField(\"service_id\", StringType(), False),\n",
    "                StructField(\"timestamp_min\", TimestampType(), False),\n",
    "                StructField(\"traffic_count\", IntegerType(), False),\n",
    "            ]\n",
    "        )\n",
    "        self.df_cached = spark.createDataFrame(emptyRDD, agg_schema)\n",
    "\n",
    "    def preprocess_df(self, X, start_time, end_time):\n",
    "        \"\"\"\n",
    "        X = (time, status_code, service_id)\n",
    "        Group by at every minute and count traffic for every service\n",
    "        \"\"\"\n",
    "        df_empty = get_empty_df(start_time, end_time)\n",
    "        \n",
    "        df_recent = X.withColumn(\n",
    "            \"timestamp_min\", date_trunc(\"minute\", col(\"timestamp\"))\n",
    "        )\n",
    "        df_recent = df_recent.groupBy([\"service_id\", \"timestamp_min\"]).agg(\n",
    "            func.count(\"status_code\").alias(\"traffic_count\")\n",
    "        )\n",
    "        df_recent = df_recent.union(df_empty)\n",
    "        df_recent = df_recent.union(self.df_cached)\n",
    "\n",
    "        df_recent = df_recent.groupBy([\"service_id\", \"timestamp_min\"]).agg(\n",
    "            func.max(\"traffic_count\").alias(\"traffic_count\")\n",
    "        )\n",
    "        self.df_cached = df_recent\n",
    "        self.df_cached = self.df_cached.cache()\n",
    "\n",
    "        return self.df_cached\n",
    "\n",
    "    def detect_anomaly(self, X, start_time, end_time):\n",
    "        df = self.preprocess_df(X, start_time, end_time)\n",
    "        df_window = df.groupBy(\n",
    "            [func.window(\"timestamp_min\", f\"{window_size_mins} minutes\"), \"service_id\"]\n",
    "        ).agg(func.sum(\"traffic_count\").alias(\"traffic_count\"))\n",
    "        df_window = df_window.withColumnRenamed(\"window\", \"timestamp_min\")\n",
    "        df_window = df_window.withColumn(\n",
    "            \"timestamp_min\", timestamp_min_udf(\"timestamp_min\")\n",
    "        )\n",
    "\n",
    "        windowSpec = (\n",
    "            Window()\n",
    "            .partitionBy(func.col(\"service_id\"))\n",
    "            .orderBy(func.col(\"timestamp_min\").cast(\"long\"))\n",
    "            .rangeBetween(-rolling_time_in_seconds(total_mins_to_look_back), -1)\n",
    "        )\n",
    "        df_window_avg = df_window.withColumn(\n",
    "            \"rolling_average\", func.avg(\"traffic_count\").over(windowSpec)\n",
    "        )\n",
    "        df_window_avg_std = df_window_avg.withColumn(\n",
    "            \"rolling_std\", func.stddev(\"traffic_count\").over(windowSpec)\n",
    "        )\n",
    "        df_anomaly = df_window_avg_std.withColumn(\n",
    "            \"is_anomaly\",\n",
    "            is_anomaly_udf(\"traffic_count\", \"rolling_average\", \"rolling_std\"),\n",
    "        )\n",
    "        df_anomaly = df_anomaly.cache()\n",
    "        return df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83508593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dict_list(df_res):\n",
    "    df_res = df_res[df_res.is_anomaly == True]\n",
    "    rows = df_res.collect()\n",
    "    dict_summ = list(map(lambda row: row.asDict(), rows))\n",
    "    return dict_summ\n",
    "\n",
    "\n",
    "def raise_alerts(dict_list, curr_window_long):\n",
    "    total_alerts_in_window = 0\n",
    "    curr_datetime = datetime.utcfromtimestamp(curr_window_long / 1000)\n",
    "    for alert_dict in dict_list:\n",
    "        if alert_dict[\"is_anomaly\"]:\n",
    "            total_alerts_in_window += 1\n",
    "            print(\n",
    "                f\"{alert_dict['service_id']} is anomalous in window {alert_dict['timestamp_min']}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Current Traffic Count: {alert_dict['traffic_count']}. Expected Traffic Count: {alert_dict['rolling_average']}\"\n",
    "            )\n",
    "\n",
    "    if total_alerts_in_window == 0 or len(dict_list) == 0:\n",
    "        print(f\"No Alerts Found in window: {curr_datetime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_detector = RollingAnomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488038ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unix_to_time(x):\n",
    "    return datetime.utcfromtimestamp(x / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the main code where we run our loop continously\n",
    "# to consume data from kafka, create batches or windows and send that\n",
    "# data to anomaly detector to classify whether we have any anomaly in the current winedow\n",
    "\n",
    "df = read_from_kafka(0, 1)\n",
    "service_df = convert_kafka_to_service_df(df)\n",
    "window_start_time = service_df.first()[\"timestamp\"]\n",
    "\n",
    "i = 1\n",
    "n = 100\n",
    "\n",
    "# Create window DataFrame\n",
    "window_df = spark.createDataFrame([], stream_schema)\n",
    "\n",
    "# Create offset counter\n",
    "offset_counter = 3000\n",
    "\n",
    "## Start Loop Here\n",
    "while True:\n",
    "    i += 1\n",
    "    if i == n:\n",
    "        break\n",
    "\n",
    "    # Create the time window\n",
    "    window_end_time = window_start_time + window_size_millisec\n",
    "    window_df_end_time = 0\n",
    "    while window_end_time > window_df_end_time:\n",
    "        df = convert_kafka_to_service_df(\n",
    "            read_from_kafka(offset_counter, offset_counter + offset_window_size)\n",
    "        )\n",
    "        window_df = window_df.union(df)\n",
    "        window_df_end_time = df.select(\"timestamp\").rdd.max()[0]\n",
    "        offset_counter = offset_counter + offset_window_size\n",
    "\n",
    "    # Filter the dataframe to contain only the sliding window values\n",
    "    # Remove values before sliding window\n",
    "    window_df = window_df.filter(window_df.timestamp >= window_start_time)\n",
    "\n",
    "    # Only send rows that are within the window\n",
    "    df_window_final = window_df.filter(window_df.timestamp <= window_end_time)\n",
    "    \n",
    "    df_window_final = preprocess_df(df_window_final)\n",
    "    df_anomalies_detected = anomaly_detector.detect_anomaly(df_window_final, start_time=window_start_time, end_time = window_end_time)\n",
    "    anomaly_result = df_to_dict_list(df_anomalies_detected)\n",
    "    raise_alerts(anomaly_result, window_start_time)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Update window start time\n",
    "    window_start_time = window_start_time + window_period_millisec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d2fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
